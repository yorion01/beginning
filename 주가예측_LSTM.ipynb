{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yorion01/beginning/blob/main/%EC%A3%BC%EA%B0%80%EC%98%88%EC%B8%A1_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIEHt-Q8IQ2g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import math\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from datetime import datetime\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/금융데이터_전처리.xlsx')"
      ],
      "metadata": {
        "id": "T1g4RuPSKF11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.set_index('Date')"
      ],
      "metadata": {
        "id": "lJve-NpeLeSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 분리"
      ],
      "metadata": {
        "id": "q7CiGxkRKEOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 특성\n",
        "feature1_list = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume','log_return']\n",
        "feature2_list = ['MA5', 'MA10', 'RASD5', 'RASD10', 'MACD', 'CCI', 'ATR', 'ub', 'middle', 'lb', 'MTM1', 'MTM3', 'ROC', 'WPR',]\n",
        "feature3_list = ['S&P500', 'SOX', 'VIX']\n",
        "feature4_list = ['next_rtn']\n",
        "\n",
        "all_feature = feature1_list + feature2_list + feature3_list + feature4_list\n",
        "\n",
        "phase_flage = '3'\n",
        "\n",
        "if phase_flage == '1':\n",
        "    train_form = datetime(2010,1,4)\n",
        "    train_to = datetime(2012,1,1)\n",
        "    val_form = datetime(2012,1,1)\n",
        "    val_to = datetime(2012,4,1)\n",
        "    test_form = datetime(2010,4,1)\n",
        "    test_to = datetime(2012,7,1)\n",
        "\n",
        "elif phase_flage == '2':\n",
        "    train_form = datetime(2012,7,1)\n",
        "    train_to = datetime(2014,7,1)\n",
        "    val_form = datetime(2014,7,1)\n",
        "    val_to = datetime(2014,10,1)\n",
        "    test_form = datetime(2014,10,1)\n",
        "    test_to = datetime(2015,1,1)\n",
        "\n",
        "else:\n",
        "    train_form = datetime(2015,1,1)\n",
        "    train_to = datetime(2017,1,1)\n",
        "    val_form = datetime(2017,1,1)\n",
        "    val_to = datetime(2017,4,1)\n",
        "    test_form = datetime(2017,4,1)\n",
        "    test_to = datetime(2017,7,1)\n",
        "\n",
        "# 학습/검증/테스트\n",
        "train_df = df.loc[train_form:train_to,all_feature].copy()\n",
        "val_df = df.loc[val_form:val_to,all_feature].copy()\n",
        "test_df = df.loc[test_form:test_to,all_feature].copy()"
      ],
      "metadata": {
        "id": "8GW6eMeEJW77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 정규화"
      ],
      "metadata": {
        "id": "cN5G0MSDKPjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def min_max_normal(tmp_df):\n",
        "    eng_list = []\n",
        "    sample_df = tmp_df.copy()\n",
        "    for x in all_feature:  # x는 각 col 중 한개\n",
        "        if x in feature4_list:\n",
        "            continue\n",
        "        series = sample_df[x].copy()\n",
        "        values = series.values   # 1*n\n",
        "        values = values.reshape((len(values),1))  # n*1\n",
        "\n",
        "        # 스케일러 생성 및 훈련\n",
        "        scaler = MinMaxScaler(feature_range=(0,1)) # 최소 0, 최대 1\n",
        "        scaler = scaler.fit(values)\n",
        "\n",
        "        # 데이터셋 정규화 및 출력\n",
        "        normalized = scaler.transform(values)\n",
        "        new_feature = '{}_normal'.format(x)\n",
        "        eng_list.append(new_feature)\n",
        "        sample_df[new_feature] = normalized\n",
        "    return sample_df,eng_list"
      ],
      "metadata": {
        "id": "PNgWrw09KSQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sample_df , eng_list = min_max_normal(train_df)\n",
        "val_sample_df , eng_list = min_max_normal(val_df)\n",
        "test_sample_df , eng_list = min_max_normal(test_df)"
      ],
      "metadata": {
        "id": "TPd-1SabKThe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 학습 데이터와 레이블 데이터 나눔\n",
        "# def create_dataset_binary(data, feature_list, step, n):    # step은 몇개를 한번에 묶어서 학습할 것인가, n은 총 col수-1(1개는 결과값)\n",
        "#     # LSTM에 넣어줄 변수 데이터 선택 - 배열로 만듬    // 데이터 수*n\n",
        "#     train_xdata = np.array(data[feature_list[0:n]])\n",
        "#     # m = 총 단계 개수 Ex. [0,1,2,,,,,,,,m]\n",
        "#     m = np.arange(len(train_xdata)-step)\n",
        "#     x,y = [], []\n",
        "#     for i in m:\n",
        "#         # 각 단계마다 사용할 학습 데이터 기간 설정\n",
        "#         a = train_xdata[i:(i+step)]\n",
        "#         x.append(a)\n",
        "#     # 신경망에 사용할 데이터 정리(3차원으로 구성)\n",
        "#     x_batch = np.reshape(np.array(x), (len(m),step,n))  # m*step*n\n",
        "\n",
        "#     # 레이블 데이터 생성(종가)\n",
        "#     train_ydata = np.array(data[[feature_list[n]]])\n",
        "\n",
        "#     # n_step 이상부터 답을 사용할 수 있다.\n",
        "#     for i in m + step:\n",
        "#         start_price = train_ydata[i-1][0]     # 전날 종가\n",
        "#         end_price = train_ydata[i][0]     # 당일 종가\n",
        "\n",
        "#         if end_price>start_price:\n",
        "#             label = 1\n",
        "#         else:\n",
        "#             label = 0\n",
        "#         y.append(label)\n",
        "#     y_batch = np.reshape(np.array(y), (-1,1))\n",
        "#     return torch.FloatTensor(x_batch).to(device), torch.FloatTensor(y_batch).to(device)\n",
        "\n",
        "# num_step = 5\n",
        "# num_unit = 200\n",
        "# n_feature = len(eng_list)-1\n",
        "\n",
        "# x_train, y_train = create_dataset_binary(train_sample_df[eng_list], eng_list, num_step, n_feature)\n",
        "# x_val, y_val = create_dataset_binary(val_sample_df[eng_list], eng_list, num_step, n_feature)\n",
        "# x_test, y_test = create_dataset_binary(test_sample_df[eng_list], eng_list, num_step, n_feature)\n",
        "\n",
        "# train = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "# val = torch.utils.data.TensorDataset(x_val, y_val)\n",
        "# test = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "\n",
        "# batch_size = 10\n",
        "# train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=batch_size, shuffle=False)\n",
        "# val_loader = torch.utils.data.DataLoader(dataset=val, batch_size=batch_size, shuffle=False)\n",
        "# test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "ta9rmd5sKVyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_sample_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ifwvoa9Q6vE",
        "outputId": "e919bdae-0013-4748-dfdb-2a9334cdcdaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "504"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_step = 5\n",
        "num_unit = 200\n",
        "n_feature = len(eng_list)-1\n",
        "\n",
        "class windowDataset(Dataset):\n",
        "    def __init__(self, data, feature_list = eng_list, step = num_step, n = n_feature):\n",
        "        self.feature_list = feature_list\n",
        "        train_xdata = np.array(data[self.feature_list[0:n]])  # 504 * 23\n",
        "        # m = 총 단계 개수 Ex. [0,1,2,,,,,,,,m]\n",
        "        m = np.arange(len(train_xdata)-step)   # (499,)\n",
        "        x,y = [], []\n",
        "        for i in m:\n",
        "            # 각 단계마다 사용할 학습 데이터 기간 설정\n",
        "            a = train_xdata[i:(i+step)]\n",
        "            x.append(a)\n",
        "        # 신경망에 사용할 데이터 정리(3차원으로 구성)\n",
        "        self.x = torch.FloatTensor(np.reshape(np.array(x), (len(m),step,n)))  # 499*5*23 -> 499 * 5 * 23\n",
        "\n",
        "        # 레이블 데이터 생성(종가)\n",
        "        train_ydata = np.array(data[[self.feature_list[n]]])\n",
        "\n",
        "        # n_step 이상부터 답을 사용할 수 있다.\n",
        "        for i in m + step:\n",
        "            start_price = train_ydata[i-1][0]     # 전날 종가\n",
        "            end_price = train_ydata[i][0]     # 당일 종가\n",
        "\n",
        "            if end_price>start_price:\n",
        "                label = 1\n",
        "            else:\n",
        "                label = 0\n",
        "            y.append(label)\n",
        "        self.y = torch.FloatTensor(np.reshape(np.array(y), (-1,1)))\n",
        "        self.len = len(self.x)\n",
        "\n",
        "    # 이거 갯수만큼 돌게 됨 -> 이거 갯수 틀리면 학습시 에러뜸!!\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    # return 값들\n",
        "    def __getitem__(self, i):\n",
        "        return self.x[i], self.y[i]\n",
        "\n"
      ],
      "metadata": {
        "id": "nIY3H4KPnsCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = windowDataset(train_sample_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, drop_last=True)\n",
        "val_dataset = windowDataset(val_sample_df)\n",
        "val_loader = DataLoader(val_dataset, batch_size=10, drop_last=True)\n",
        "test_dataset = windowDataset(test_sample_df)\n",
        "test_loader = DataLoader(test_dataset, batch_size=10, drop_last=True)"
      ],
      "metadata": {
        "id": "QAZuG1egrCF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in train_loader:\n",
        "  print(\"X 크기 : {}\".format(x.shape[1:]))\n",
        "  print(\"y 크기 : {}\".format(y.shape[1:]))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGg614q9W0kJ",
        "outputId": "53ca69d5-3ea6-4f7b-a28e-e5f39503c7d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X 크기 : torch.Size([5, 23])\n",
            "y 크기 : torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 & 모델에 device 붙임!!!\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'{device} is available')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dagboj8mYrwc",
        "outputId": "63f41b32-2947-4f17-835d-233590f0f782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 생성하기"
      ],
      "metadata": {
        "id": "7k9h5Y0kL8tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "vtu8bZOLMPgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 23\n",
        "num_layers = 2\n",
        "hidden_size = 200\n",
        "sequence_length = 5"
      ],
      "metadata": {
        "id": "YVLcw2MSVrgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_model(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, sequence_length, num_layers, device):\n",
        "    super(LSTM_model, self).__init__()\n",
        "    self.device = device\n",
        "    self.hidden_size = hidden_size   # 5\n",
        "    self.num_layers = num_layers     # 5\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)  # batch_size * 5(일수) * 23(column 수) -> batch_size * 5 * 5\n",
        "    self.fc = nn.Sequential(nn.Linear(hidden_size * sequence_length, 1), nn.Sigmoid())\n",
        "\n",
        "  # x : 20(batch_size) * 5 * 23,\n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(self.num_layers, x.size()[0], self.hidden_size).to(self.device) # 초기 hidden state 설정하기. (layer 높이, 배치사이즈, )  num_layers * batch_size * hidden_size\n",
        "    c0 = torch.zeros(self.num_layers, x.size()[0], self.hidden_size).to(self.device) # 이거 추가 -> rnn과 다른점  num_layers * batch_size * hidden_size\n",
        "    out, hidden = self.lstm(x, (h0,c0)) # out: RNN의 마지막 레이어로부터 나온 output feature 를 반환한다. hn: hidden state를 반환한다.\n",
        "    out = out.reshape(out.shape[0], -1) # many to many 전략\n",
        "    out = self.fc(out)  # 이걸 넣으면 many to one 됨\n",
        "    return out\n",
        "\n",
        "\n",
        "model = LSTM_model(input_size=input_size,\n",
        "                   hidden_size=hidden_size,\n",
        "                   sequence_length=sequence_length,\n",
        "                   num_layers=num_layers,\n",
        "                   device=device).to(device)\n",
        "\n",
        "print('작동하는지 실험')\n",
        "basic_data = torch.rand((1,5,23))\n",
        "model(basic_data)"
      ],
      "metadata": {
        "id": "Cvp-M-33ON0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d9e17a-6a68-4b71-c885-8e2fd8a88c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "작동하는지 실험\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4965]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_model(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, sequence_length, num_layers, device):\n",
        "    super(LSTM_model, self).__init__()\n",
        "    self.device = device\n",
        "    self.hidden_size = hidden_size   # 200\n",
        "    self.num_layers = num_layers     # 5\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)  # batch_size * 5(일수) * 23(column 수) -> batch_size * 5 * 200\n",
        "    self.lstm1 = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.lstm2 = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.lstm3 = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.lstm4 = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.batch = nn.BatchNorm1d(5)\n",
        "    self.dropout = nn.Dropout(p=0.25)\n",
        "    self.fc = nn.Sequential(nn.Linear(hidden_size * sequence_length, 1), nn.Sigmoid())\n",
        "\n",
        "  # x : 20(batch_size) * 5 * 23,\n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(self.num_layers, x.size()[0], self.hidden_size).to(self.device) # 초기 hidden state 설정하기. (layer 높이, 배치사이즈, )  5 * batch_size * 5\n",
        "    c0 = torch.zeros(self.num_layers, x.size()[0], self.hidden_size).to(self.device) # 이거 추가 -> rnn과 다른점  5 * batch_size * 5\n",
        "    out0, hidden = self.lstm(x, (h0,c0)) # out: RNN의 마지막 레이어로부터 나온 output feature 를 반환한다. hn: hidden state를 반환한다.\n",
        "    out0 = self.batch(out0)\n",
        "\n",
        "    for i in range(1,5):\n",
        "      globals()['h{}'.format(i)] = torch.zeros(self.num_layers, x.size()[0], self.hidden_size).to(self.device)\n",
        "      globals()['c{}'.format(i)] = torch.zeros(self.num_layers, x.size()[0], self.hidden_size).to(self.device)\n",
        "\n",
        "    out1, _ = self.lstm1(out0,(h1,c1))\n",
        "    out1 = self.dropout(out1)\n",
        "    out2, _ = self.lstm2(out1,(h2,c2))\n",
        "    out2 = self.batch(out2)\n",
        "    out3, _ = self.lstm3(out2,(h3,c3))\n",
        "    out3 = self.dropout(out3)\n",
        "    out4, _ = self.lstm4(out3,(h4,c4))\n",
        "\n",
        "    out = out4.reshape(out4.shape[0], -1) # many to many 전략\n",
        "    out = self.fc(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "model = LSTM_model(input_size=input_size,\n",
        "                   hidden_size=hidden_size,\n",
        "                   sequence_length=sequence_length,\n",
        "                   num_layers=num_layers,\n",
        "                   device=device).to(device)\n",
        "\n",
        "print('작동하는지 실험')\n",
        "basic_data = torch.rand((1,5,23))\n",
        "model(basic_data)\n"
      ],
      "metadata": {
        "id": "obbJExM82lI_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdebfec1-a2ee-4d6b-8eb2-f5d480bfc6e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "작동하는지 실험\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5060]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhPd0Po8Waa3",
        "outputId": "562a7ba3-9912-4220-f5ca-d418d21e1767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM_model(\n",
              "  (lstm): LSTM(23, 200, num_layers=2, batch_first=True)\n",
              "  (lstm1): LSTM(200, 200, num_layers=2, batch_first=True)\n",
              "  (lstm2): LSTM(200, 200, num_layers=2, batch_first=True)\n",
              "  (lstm3): LSTM(200, 200, num_layers=2, batch_first=True)\n",
              "  (lstm4): LSTM(200, 200, num_layers=2, batch_first=True)\n",
              "  (batch): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=1000, out_features=1, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "\n",
        "lr = 1e-3\n",
        "num_epochs = 5\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "JzdlaxfMWi2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 학습하기"
      ],
      "metadata": {
        "id": "SCDW5-2sWwJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_graph = [] # 그래프 그릴 목적인 loss.\n",
        "n = len(train_loader)\n",
        "best_val_loss = np.inf\n",
        "best_val_acc = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    num = 0\n",
        "    for inputs, labels in train_loader:  # dataloader 함수에 def __len__에 return 값만큼 반복함\n",
        "\n",
        "              inputs = inputs.to(device)\n",
        "              labels = labels.to(device)\n",
        "              out = model(inputs)\n",
        "              loss = criterion(out, labels)\n",
        "\n",
        "              optimizer.zero_grad() #\n",
        "              loss.backward() # loss가 최소가 되게하는\n",
        "              optimizer.step() # 어떤 방법으로 learning rate를 움직일거이냐\n",
        "              running_loss += loss.item() # 한 배치의 loss 더해주고,\n",
        "    print(\"train\")\n",
        "    print(running_loss)\n",
        "    with torch.no_grad():\n",
        "              print(\"Calculating validation results...\")\n",
        "              model.eval()\n",
        "              val_loss_items = []\n",
        "              val_acc_items = []\n",
        "              figure = None\n",
        "              for inputs, labels in val_loader:\n",
        "                  inputs = inputs.to(device)\n",
        "                  labels = labels.to(device)\n",
        "\n",
        "                  outs = model(inputs)\n",
        "                  preds = torch.argmax(outs, dim=-1)\n",
        "\n",
        "                  loss_item = criterion(outs, labels).item()\n",
        "                  acc_item = (labels == preds).sum().item()\n",
        "                  val_loss_items.append(loss_item)\n",
        "\n",
        "\n",
        "              val_loss = np.sum(val_loss_items)\n",
        "              val_acc = np.sum(val_acc_items) / len(val_loader)\n",
        "              best_val_loss = min(best_val_loss, val_loss)\n",
        "              # if val_acc > best_val_acc:\n",
        "              #     print(f\"New best model for val accuracy : {val_acc:4.2%}! saving the best model..\")\n",
        "              #     # torch.save(model.module.state_dict(), f\"{save_dir}/best.pth\")\n",
        "              #     best_val_acc = val_acc\n",
        "              # torch.save(model.module.state_dict(), f\"{save_dir}/last.pth\")\n",
        "              print(\"val\")\n",
        "              print(\n",
        "                  f\"loss: {val_loss:4.8} || \"\n",
        "                  f\"best loss: {best_val_loss:4.2}\"\n",
        "              )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXdGaxu2WkqA",
        "outputId": "31362fe9-0f43-480a-e36c-bfd436726ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "12.282976388931274\n",
            "Calculating validation results...\n",
            "val\n",
            "loss: 1.2521369 || best loss:  1.3\n",
            "train\n",
            "12.254771769046783\n",
            "Calculating validation results...\n",
            "val\n",
            "loss: 1.2516711 || best loss:  1.3\n",
            "train\n",
            "12.248005673289299\n",
            "Calculating validation results...\n",
            "val\n",
            "loss: 1.2517587 || best loss:  1.3\n",
            "train\n",
            "12.244600385427475\n",
            "Calculating validation results...\n",
            "val\n",
            "loss: 1.2517434 || best loss:  1.3\n",
            "train\n",
            "12.24311175942421\n",
            "Calculating validation results...\n",
            "val\n",
            "loss: 1.2518128 || best loss:  1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7QDBZd4nXhkJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}